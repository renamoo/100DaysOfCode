# Today's Theme: Big Data

## What I have learned today:

### Material: [Cross Validation](https://www.kaggle.com/alexisbcook/cross-validationhttps://www.kaggle.com/alexisbcook/cross-validation)
- When the dataset is not so big, we may better to take cross-validation strategy. Given that we have to use at least 20% of dataset as validation, if the dataset is not so big, we might get the model which can perform better only on such a small data. To avoid this situation, cross-validation is often used. It divide the data set for several parts evently and use one of them as a validation and the others as training data. We rotate the validation data so that every parts would be validation once. There is no clear standard to decide how many data is "big" or "small", but if your modeling takes only a few minutes, it might be better to switch to cross-validation. 

### Material: [cocoda](https://cocoda-design.com)
- Started to learn about design :)
    
## What I have read today:
### Material: [Data Processing Pipelines](https://landing.google.com/sre/sre-book/chapters/data-processing-pipelines/)
- Mostly, several jobs are conbined in the pipeline. Managing dependencies matters very much when there are inconsistent or corrupted data. 
- In google, there are several steps before new version of pipeline is deployed: local testing, testing with production data, behavior cheking on staging and etc.

## Tomorrow
I will move on to [](https://www.kaggle.com/alexisbcook/xgboost)
