# Today's Theme: Big Data

## What I have learned today:

### Material: [XGBoost](https://www.kaggle.com/alexisbcook/xgboost)
- I (We) have been using RandomForestModel, which is one of emsemble methods. Today I leared anohter ensemble method called XGBoost. It is the latest, strong model. 
- It make a model => test it => fill the loss => add new model => and goes round and round...
- When we initialize XGBRegressor, we can set several variables. Let's just mentions some crucial variables. First, n_estimaters defined how many times we iterate over. It is mostly 100 to 1000. Second, early_stopping_round allows us to stop iterating in the best point. However, if we stop once accuracy goes down, it might be a technical error due to the dataset. Therefore, we can specifies how many times we allow the rate decreasing. This is early_stopping_round. Lastly, we can set learning_rate. It sums up several models so it can save time to create the model. Generaly, high n_estimaters and low learning_rate (default is 0.1) brings the best performance.

## What I have read today:
### Material: [Effective TroubleShooting](https://landing.google.com/sre/sre-book/chapters/effective-troubleshooting/)

## Tomorrow
I will move on to [Data Leakage](https://www.kaggle.com/alexisbcook/data-leakage)
